{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob,pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype = \"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureExtractor(audio):\n",
    "    librosa_audio_data, librosa_sample_rate=librosa.load(audio)\n",
    "    mfccs_features= librosa.feature.mfcc(y=librosa_audio_data, sr= librosa_sample_rate,n_mfcc=20)\n",
    "    mfccs_scaled_features=np.mean(mfccs_features.T,axis=0)                             \n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(test_size=0.2):\n",
    "#     x,y = [],[]\n",
    "#     for file in glob.glob(r\"D:\\BTP\\infant cry\\donateacry_corpus_cleaned_and_updated_data\\*\"):\n",
    "#         label = os.path.basename(file)\n",
    "#         for audio in glob.glob(file+\"\\*.wav\"):\n",
    "# #             print(audio)\n",
    "#             feature = featureExtractor(audio)\n",
    "#             x.append(feature)\n",
    "#             y.append(label)\n",
    "#     return train_test_split(np.array(x), np.array(y), test_size=test_size, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(x_train),len(y_train),len(x_test),len(y_test))\n",
    "# print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features=[]\n",
    "def load_data():\n",
    "    c=0\n",
    "    for file in glob.glob(r\"D:\\BTP\\infant cry\\donateacry_corpus_cleaned_and_updated_data\\*\"):\n",
    "        label = os.path.basename(file)\n",
    "        for audio in glob.glob(file+\"\\*.wav\"):\n",
    "#             print(audio)\n",
    "            if(label=='hungry'):\n",
    "                if(c>200):\n",
    "                    continue\n",
    "                c+=1\n",
    "            feature = featureExtractor(audio)\n",
    "            extracted_features.append([feature,label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-497.25443, 110.52981, -51.0884, -15.25888, 1...</td>\n",
       "      <td>belly_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-497.26553, 151.22801, -56.617985, -1.158033,...</td>\n",
       "      <td>belly_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-421.2681, 153.3781, -55.931305, -7.050015, 6...</td>\n",
       "      <td>belly_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-383.0191, 164.00575, -70.79439, -8.87635, 8....</td>\n",
       "      <td>belly_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-310.78342, 171.10025, -66.29349, 4.647664, 1...</td>\n",
       "      <td>belly_pain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature       class\n",
       "0  [-497.25443, 110.52981, -51.0884, -15.25888, 1...  belly_pain\n",
       "1  [-497.26553, 151.22801, -56.617985, -1.158033,...  belly_pain\n",
       "2  [-421.2681, 153.3781, -55.931305, -7.050015, 6...  belly_pain\n",
       "3  [-383.0191, 164.00575, -70.79439, -8.87635, 8....  belly_pain\n",
       "4  [-310.78342, 171.10025, -66.29349, 4.647664, 1...  belly_pain"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(276, 2)\n"
     ]
    }
   ],
   "source": [
    "print(extracted_features_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_file_path='belly_pain.wav'\n",
    "# mfcc=featureExtractor(audio_file_path)\n",
    "# print(mfcc)\n",
    "# print(mfcc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "Y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(276, 20) (276,)\n",
      "['belly_pain' 'belly_pain' 'belly_pain' 'belly_pain' 'belly_pain'\n",
      " 'belly_pain' 'belly_pain' 'belly_pain' 'belly_pain' 'belly_pain'\n",
      " 'belly_pain' 'belly_pain' 'belly_pain' 'belly_pain' 'belly_pain'\n",
      " 'belly_pain' 'burping' 'burping' 'burping' 'burping' 'burping' 'burping'\n",
      " 'burping' 'burping' 'discomfort' 'discomfort' 'discomfort' 'discomfort'\n",
      " 'discomfort' 'discomfort' 'discomfort' 'discomfort' 'discomfort'\n",
      " 'discomfort' 'discomfort' 'discomfort' 'discomfort' 'discomfort'\n",
      " 'discomfort' 'discomfort' 'discomfort' 'discomfort' 'discomfort'\n",
      " 'discomfort' 'discomfort' 'discomfort' 'discomfort' 'discomfort'\n",
      " 'discomfort' 'discomfort' 'discomfort' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry'\n",
      " 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'hungry' 'tired' 'tired'\n",
      " 'tired' 'tired' 'tired' 'tired' 'tired' 'tired' 'tired' 'tired' 'tired'\n",
      " 'tired' 'tired' 'tired' 'tired' 'tired' 'tired' 'tired' 'tired' 'tired'\n",
      " 'tired' 'tired' 'tired' 'tired']\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,Y.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(276,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "y=to_categorical(labelencoder.fit_transform(Y))\n",
    "# Y=np.array(pd.get_dummies(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(276, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 20)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 5)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 20)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 5)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# BUILDING MODEL\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN\n",
    "model=Sequential()\n",
    "#first layer\n",
    "model.add(Dense(100,input_shape=(20,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#second layer\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "          \n",
    "# final layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 100)               2100      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 23,305\n",
      "Trainable params: 23,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7/7 [==============================] - 3s 259ms/step - loss: 41.6149 - accuracy: 0.3871 - val_loss: 14.9847 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 14.98472, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 32.6679 - accuracy: 0.5508 - val_loss: 13.6108 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00002: val_loss improved from 14.98472 to 13.61083, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 28.5419 - accuracy: 0.5483 - val_loss: 7.8117 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00003: val_loss improved from 13.61083 to 7.81166, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 23.8673 - accuracy: 0.4877 - val_loss: 7.5440 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00004: val_loss improved from 7.81166 to 7.54399, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 21.3438 - accuracy: 0.5004 - val_loss: 7.5279 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00005: val_loss improved from 7.54399 to 7.52792, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 24.5620 - accuracy: 0.5243 - val_loss: 6.0264 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00006: val_loss improved from 7.52792 to 6.02641, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 15.6600 - accuracy: 0.5730 - val_loss: 4.5293 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00007: val_loss improved from 6.02641 to 4.52934, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - ETA: 0s - loss: 14.6037 - accuracy: 0.625 - 0s 27ms/step - loss: 16.0142 - accuracy: 0.5168 - val_loss: 3.9845 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.52934 to 3.98448, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 17.9416 - accuracy: 0.5219 - val_loss: 3.2277 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.98448 to 3.22768, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 14.1962 - accuracy: 0.5771 - val_loss: 2.1092 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.22768 to 2.10920, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 12.1497 - accuracy: 0.5414 - val_loss: 2.0231 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.10920 to 2.02312, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 9.8725 - accuracy: 0.5576 - val_loss: 2.7574 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.02312\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 9.3580 - accuracy: 0.5793 - val_loss: 2.9005 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.02312\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 10.0928 - accuracy: 0.6078 - val_loss: 2.4172 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.02312\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 8.4828 - accuracy: 0.5579 - val_loss: 1.5764 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.02312 to 1.57642, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 8.4291 - accuracy: 0.4676 - val_loss: 1.3125 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.57642 to 1.31252, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 8.4627 - accuracy: 0.4931 - val_loss: 1.2639 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.31252 to 1.26388, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 6.2974 - accuracy: 0.5670 - val_loss: 1.2087 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.26388 to 1.20866, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 6.2718 - accuracy: 0.6022 - val_loss: 1.1447 - val_accuracy: 0.7679\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.20866 to 1.14470, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 4.6924 - accuracy: 0.5550 - val_loss: 1.0833 - val_accuracy: 0.7321\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.14470 to 1.08334, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 6.0687 - accuracy: 0.5358 - val_loss: 1.0821 - val_accuracy: 0.7321\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.08334 to 1.08213, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 4.9880 - accuracy: 0.5021 - val_loss: 1.0346 - val_accuracy: 0.7679\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.08213 to 1.03457, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 4.8408 - accuracy: 0.5121 - val_loss: 1.0292 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.03457 to 1.02925, saving model to saved_models\\audio_classification.hdf5\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 5.0523 - accuracy: 0.4757 - val_loss: 1.0316 - val_accuracy: 0.7321\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.02925\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4.4671 - accuracy: 0.5497 - val_loss: 1.0537 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.02925\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4.5663 - accuracy: 0.5255 - val_loss: 1.0839 - val_accuracy: 0.7321\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.02925\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 4.8514 - accuracy: 0.5519 - val_loss: 1.1179 - val_accuracy: 0.6964\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.02925\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 3.4358 - accuracy: 0.5186 - val_loss: 1.1249 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.02925\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 3.3191 - accuracy: 0.5215 - val_loss: 1.1200 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.02925\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 3.1283 - accuracy: 0.5311 - val_loss: 1.1365 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.02925\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.0544 - accuracy: 0.5532 - val_loss: 1.1440 - val_accuracy: 0.6964\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.02925\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 3.0730 - accuracy: 0.5378 - val_loss: 1.1440 - val_accuracy: 0.6964\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.02925\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 3.2267 - accuracy: 0.5814 - val_loss: 1.1653 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.02925\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.8977 - accuracy: 0.5991 - val_loss: 1.1588 - val_accuracy: 0.7321\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.02925\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 2.2811 - accuracy: 0.5664 - val_loss: 1.1542 - val_accuracy: 0.6786\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.02925\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 3.1565 - accuracy: 0.5531 - val_loss: 1.1669 - val_accuracy: 0.7321\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.02925\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.0399 - accuracy: 0.5932 - val_loss: 1.1668 - val_accuracy: 0.7321\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.02925\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 1.9756 - accuracy: 0.6245 - val_loss: 1.1715 - val_accuracy: 0.6607\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.02925\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 2.4581 - accuracy: 0.5629 - val_loss: 1.1957 - val_accuracy: 0.5893\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.02925\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 2.0578 - accuracy: 0.5564 - val_loss: 1.2081 - val_accuracy: 0.6429\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.02925\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 1.8219 - accuracy: 0.5989 - val_loss: 1.1972 - val_accuracy: 0.6964\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.02925\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.8079 - accuracy: 0.5539 - val_loss: 1.1757 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.02925\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 1.7780 - accuracy: 0.6075 - val_loss: 1.1577 - val_accuracy: 0.8036\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.02925\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.8333 - accuracy: 0.5678 - val_loss: 1.1470 - val_accuracy: 0.8036\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.02925\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.5368 - accuracy: 0.5902 - val_loss: 1.1360 - val_accuracy: 0.8036\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.02925\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.9647 - accuracy: 0.6025 - val_loss: 1.1365 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.02925\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.6558 - accuracy: 0.5756 - val_loss: 1.1391 - val_accuracy: 0.7679\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.02925\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 1.7196 - accuracy: 0.6269 - val_loss: 1.1435 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.02925\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 1.5573 - accuracy: 0.5797 - val_loss: 1.1374 - val_accuracy: 0.7679\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.02925\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 1.5713 - accuracy: 0.5768 - val_loss: 1.1339 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.02925\n",
      "Training completed in time:  0:00:15.991936\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "num_epochs=50\n",
    "num_batch_size=32\n",
    "\n",
    "checkpointer=ModelCheckpoint(filepath='saved_models/audio_classification.hdf5',\n",
    "                             verbose=1,save_best_only=True)\n",
    "start=datetime.now()\n",
    "model.fit(X_train,y_train,batch_size=num_batch_size,epochs=num_epochs,\n",
    "          validation_data=(X_test,y_test),callbacks=[checkpointer] )\n",
    "duration=datetime.now() - start\n",
    "print(\"Training completed in time: \",duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.1338931322097778\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "loss,test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(\"loss: \",loss)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajeen\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['hungry'], dtype='<U10')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename='burping.wav'\n",
    "prediction_feature=featureExtractor(filename).reshape(1,-1)\n",
    "# model.predict_classes(prediction_feature)\n",
    "predicted_label=model.predict_classes(prediction_feature)\n",
    "print(predicted_label)\n",
    "predcited_class=labelencoder.inverse_transform(predicted_label)\n",
    "predcited_class\n",
    "# (model.predict(prediction_feature) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test) \n",
    "y_pred = np.argmax(y_pred, axis = 1) \n",
    "label = np.argmax(y_test,axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.80      0.93      0.86        44\n",
      "           4       0.20      0.25      0.22         4\n",
      "\n",
      "    accuracy                           0.75        56\n",
      "   macro avg       0.20      0.24      0.22        56\n",
      "weighted avg       0.65      0.75      0.69        56\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rajeen\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,f1_score\n",
    "print(classification_report(y_true=label,y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1-score per class:  [0.         0.         0.         0.86315789 0.22222222]\n",
      "The f1-score :  0.6940685045948204\n"
     ]
    }
   ],
   "source": [
    "f1_score_per_class_validation = f1_score(y_true=label,y_pred=y_pred,average=None) \n",
    "print(\"The f1-score per class: \",f1_score_per_class_validation)\n",
    "print(\"The f1-score : \",f1_score(y_true=label,y_pred=y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
